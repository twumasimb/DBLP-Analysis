{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import preprocessing as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/dblp_data.txt\"\n",
    "# # # Process and save the data as a pickle file\n",
    "ps.parse_and_save_paper_data(path, \"./data/dblp_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1632444"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "dataset = pickle.load(open(\"./data/dblp_data.pkl\", \"rb\"))\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'OQL[C++]: Extending C++ with an Object Query Capability.',\n",
       " 'authors': ['JosÃ© A. Blakeley'],\n",
       " 'year': 1995,\n",
       " 'venue': 'Modern Database Systems'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1630752"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only keep the data with all the values\n",
    "dataset = [data for data in dataset if all(value != '' for value in data.values())]\n",
    "dataset = [data for data in dataset if all(attr in data for attr in ['title', 'authors', 'year', 'venue'])]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Year: 2011\n",
      "First Year: 1936\n",
      "Num without a year: 162810\n"
     ]
    }
   ],
   "source": [
    "max_year = max([data['year'] for data in dataset])\n",
    "\n",
    "years = []\n",
    "min_year = sorted([data['year'] for data in dataset])\n",
    "for i in min_year:\n",
    "    if i > 0:\n",
    "        years.append(i)\n",
    "\n",
    "print(\"Latest Year:\", max_year)\n",
    "print(\"First Year:\", min(years))\n",
    "print(\"Num without a year:\", len(min_year) - len(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1371"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample by venue - Only keep papers published in one of the following conference\n",
    "venues = ['VLDB', 'ICDE', 'ICDT', 'EDBT', 'PODS', 'SIGMOD Conference', 'ICML', 'ECML', 'COLT', 'UAI', 'SODA', 'STOC', 'FOCS', 'STACS', 'KDD', 'ICDM', 'PKDD', 'WWW', 'SDM']\n",
    "dataset = ps.sample_by_venue(dataset, venues)\n",
    "dataset = ps.sample_date(dataset, 2010, 2010)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data for each author\n",
    "authors = set()\n",
    "for data in dataset:\n",
    "    if 'authors' in data:\n",
    "        authors.update(data['authors'][0].split(','))\n",
    "\n",
    "unique_authors = list(authors)\n",
    "\n",
    "# Extract the author data\n",
    "dataset = ps.get_author_data(unique_authors, dataset)\n",
    "# clean empty values from the dataset\n",
    "dataset = [data for data in dataset if all(value != '' or None for value in data.values())]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'Alin Deutsch',\n",
      " 'coauthors': ['Yannis Katsis',\n",
      "               'Vasilis Vassalos',\n",
      "               'Yannis Papakonstantinou',\n",
      "               'Richard Hull',\n",
      "               'Avinash Vyas',\n",
      "               'Kevin Keliang Zhao',\n",
      "               'Emiran Curtmola',\n",
      "               'Divesh Srivastava',\n",
      "               'K. K. Ramakrishnan'],\n",
      " 'id': 2208,\n",
      " 'num_of_papers': 3,\n",
      " 'papers': {'Inconsistency resolution in online databases.',\n",
      "            'Load-balanced query dissemination in privacy-aware online '\n",
      "            'communities.',\n",
      "            'Policy-aware sender anonymity in location based services.'},\n",
      " 'venue_dates': {'ICDE': 2010, 'SIGMOD Conference': 2010},\n",
      " 'venue_papers': {'ICDE': 2, 'SIGMOD Conference': 1},\n",
      " 'venues': ['SIGMOD Conference', 'ICDE']}\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "pp.pprint(dataset[139])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a network of authors, add a weighted edge if they have authored the same paper or have published at the same conference. \n",
    "# # Remove all self loops from the network. \n",
    "\n",
    "# # Initialize an empty graph\n",
    "# G = nx.Graph()\n",
    "\n",
    "# # Precompute the number of papers for each author\n",
    "# num_papers = {author: ps.get_num_papers(dataset, author) for author in unique_authors}\n",
    "\n",
    "# # Iterate over the dataset\n",
    "# for data in dataset:\n",
    "#     author = data['author']\n",
    "#     coauthors = data['coauthors']\n",
    "#     num_of_papers = data['num_of_papers']\n",
    "#     venue_dates = data['venue_dates']\n",
    "\n",
    "#     # Count the occurrences of each coauthor\n",
    "#     coauthor_counts = Counter(coauthors)\n",
    "\n",
    "#     # Add edges between the author and coauthors\n",
    "#     for coauthor, count in coauthor_counts.items():\n",
    "#         # Only put an edge between two nodes if they have two or more papers together\n",
    "#         if count > 1:\n",
    "#             # Calculate the weight of the edge\n",
    "#             weight = round(count / (num_of_papers + num_papers[coauthor]), 4) * 100\n",
    "#             if not G.has_edge(author, coauthor):\n",
    "#                 G.add_edge(author, coauthor, weight=weight)\n",
    "\n",
    "#     # # Add edges based on venue and year\n",
    "#     # for data2 in dataset:\n",
    "#     #     author2 = data2['author']\n",
    "#     #     venue_dates2 = data2['venue_dates']\n",
    "\n",
    "#     #     # Check if they published at the same venue in the same year\n",
    "#     #     for venue, year in venue_dates.items():\n",
    "#     #         if venue in venue_dates2 and venue_dates2[venue] == year:\n",
    "#     #             if author != author2:\n",
    "#     #                 # Calculate the weight of the edge\n",
    "#     #                 common_publications = sum(1 for data in dataset if data['author'] in {author, author2} and venue in data['venues'] and data['venue_dates'][venue] == year)\n",
    "#     #                 weight = round(common_publications / (num_papers[author] + num_papers[author2]), 4) * 50\n",
    "                    \n",
    "#     #                 if G.has_edge(author, author2):\n",
    "#     #                     # If the edge exists, add the weight to the existing weight\n",
    "#     #                     G[author][author2]['weight'] += weight\n",
    "#     #                 else:\n",
    "#     #                     # If the edge doesn't exist, create a new edge with the weight\n",
    "#     #                     G.add_edge(author, author2, weight=weight)\n",
    "\n",
    "# # Remove self-loops\n",
    "# G.remove_edges_from(nx.selfloop_edges(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a network of authors, add a weighted edge if they have authored the same paper or have published at the same conference. \n",
    "# Remove all self loops from the network. \n",
    "\n",
    "# Initialize an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Precompute the number of papers for each author\n",
    "num_papers = {author: ps.get_num_papers(dataset, author) for author in unique_authors}\n",
    "\n",
    "# Iterate over the dataset\n",
    "for data in dataset:\n",
    "    author = data['author']\n",
    "    coauthors = data['coauthors']\n",
    "    num_of_papers = data['num_of_papers']\n",
    "    venue_dates = data['venue_dates']\n",
    "\n",
    "    # Count the occurrences of each coauthor\n",
    "    coauthor_counts = Counter(coauthors)\n",
    "\n",
    "    # Add edges between the author and coauthors\n",
    "    for coauthor, count in coauthor_counts.items():\n",
    "        # Only put an edge between two nodes if they have two or more papers together\n",
    "        if count > 1:\n",
    "            # Calculate the weight of the edge\n",
    "            weight = round(1 - (count / (num_of_papers + num_papers[coauthor])), 4) * 100 # Using the Jaccard distance: Minimization problem\n",
    "            if not G.has_edge(author, coauthor):\n",
    "                G.add_edge(author, coauthor, weight=weight)\n",
    "\n",
    "min_weight = min([data['weight'] for _, _, data in G.edges(data=True)])\n",
    "\n",
    "# Create a dictionary to map venues to authors\n",
    "venue_to_authors = {}\n",
    "\n",
    "# Populate the dictionary with authors for each venue\n",
    "for data in dataset:\n",
    "    author = data['author']\n",
    "    venues = data['venues']\n",
    "    \n",
    "    for venue in venues:\n",
    "        if venue not in venue_to_authors:\n",
    "            venue_to_authors[venue] = set()\n",
    "        venue_to_authors[venue].add(author)\n",
    "\n",
    "# Add edges between authors who published in the same venue\n",
    "for authors in venue_to_authors.values():\n",
    "    authors = list(authors)\n",
    "    for i in range(len(authors)):\n",
    "        for j in range(i + 1, len(authors)):\n",
    "            author1 = authors[i]\n",
    "            author2 = authors[j]\n",
    "            if not G.has_edge(author1, author2):\n",
    "                G.add_edge(author1, author2, weight=round(min_weight/10,4))\n",
    "\n",
    "# Remove self-loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.33\n"
     ]
    }
   ],
   "source": [
    "min_weight = min([data['weight'] for _, _, data in G.edges(data=True)])\n",
    "print(min_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to the nodes to put them in teams.\n",
    "G = G.copy()\n",
    "# Convert the database to a dictionary for faster lookup\n",
    "database_dict = {item['author']: item for item in dataset}\n",
    "\n",
    "for node in G.nodes():\n",
    "    # Get the corresponding item from the database\n",
    "    item = database_dict.get(node)\n",
    "    if item is not None:\n",
    "        # Assign the label to the node\n",
    "        G.nodes[node]['label'] = ps.assign_labels(item)\n",
    "\n",
    "# Remove nodes without a label attribute from the Graph\n",
    "nodes_without_label = [node for node in G.nodes if 'label' not in G.nodes[node]]\n",
    "G.remove_nodes_from(nodes_without_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G.nodes['Xiaoyong Du']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_component = max(nx.connected_components(G), key=len)\n",
    "subgraph = G.subgraph(largest_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraph.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DB': 115, 'AI': 21, 'DM': 49, 'T': 33}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = nx.get_node_attributes(subgraph, 'label')\n",
    "label_counts = dict(Counter(label_counts.values()))\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save network G in a pickle file\n",
    "with open('./networks/min_network.pkl', 'wb') as file:\n",
    "    pickle.dump(subgraph, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
