{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import preprocessing as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/dblp_data.txt\"\n",
    "# # Process and save the data as a pickle file\n",
    "dataset = ps.parse_and_save_paper_data(path, \"./data/dblp_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the dataset\n",
    "# dataset = pickle.load(open(\"./data/dblp_data.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'OQL[C++]: Extending C++ with an Object Query Capability.',\n",
       " 'authors': ['JosÃ© A. Blakeley'],\n",
       " 'year': 1995,\n",
       " 'venue': 'Modern Database Systems'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the data with all the values\n",
    "dataset = [data for data in dataset if all(value != '' for value in data.values())]\n",
    "dataset = [data for data in dataset if all(attr in data for attr in ['title', 'authors', 'year', 'venue'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29051"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample by venue - Only keep papers published in one of the following conference\n",
    "venues = ['VLDB', 'ICDE', 'ICDT', 'EDBT', 'PODS', 'SIGMOD Conference', 'ICML', 'ECML', 'COLT', 'UAI', 'SODA', 'STOC', 'FOCS', 'STACS', 'KDD', 'ICDM', 'PKDD', 'WWW', 'SDM']\n",
    "dataset = ps.sample_by_venue(dataset, venues)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10113"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data for each author\n",
    "authors = set()\n",
    "for data in dataset:\n",
    "    if 'authors' in data:\n",
    "        authors.update(data['authors'][0].split(','))\n",
    "\n",
    "unique_authors = list(authors)\n",
    "\n",
    "# Extract the author data\n",
    "dataset = ps.get_author_data(unique_authors, dataset)\n",
    "# clean empty values from the dataset\n",
    "dataset = [data for data in dataset if all(value != '' or None for value in data.values())]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 103,\n",
       " 'author': 'Angus Macintyre',\n",
       " 'coauthors': ['Marek Karpinski', 'Eduardo D. Sontag'],\n",
       " 'venues': ['STOC'],\n",
       " 'papers': {'Finiteness results for sigmoidal \"neural\" networks.',\n",
       "  'Polynomial bounds for VC dimension of sigmoidal neural networks.'},\n",
       " 'num_of_papers': 2,\n",
       " 'venue_papers': {'STOC': 2},\n",
       " 'venue_dates': {'STOC': 1993}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a network of authors, add a weighted edge if they have authored the same paper or have published at the same conference. \n",
    "# Remove all self loops from the network. \n",
    "\n",
    "# Initialize an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Precompute the number of papers for each author\n",
    "num_papers = {author: ps.get_num_papers(dataset, author) for author in unique_authors}\n",
    "\n",
    "# Iterate over the dataset\n",
    "for data in dataset:\n",
    "    author = data['author']\n",
    "    coauthors = data['coauthors']\n",
    "    num_of_papers = data['num_of_papers']\n",
    "    venue_dates = data['venue_dates']\n",
    "\n",
    "    # Count the occurrences of each coauthor\n",
    "    coauthor_counts = Counter(coauthors)\n",
    "\n",
    "    # Add edges between the author and coauthors\n",
    "    for coauthor, count in coauthor_counts.items():\n",
    "        # Only put an edge between two nodes if they have two or more papers together\n",
    "        if count > 1:\n",
    "            # Calculate the weight of the edge\n",
    "            weight = round(count / (num_of_papers + num_papers[coauthor]), 4) * 100\n",
    "            if not G.has_edge(author, coauthor):\n",
    "                G.add_edge(author, coauthor, weight=weight)\n",
    "\n",
    "    # Add edges based on venue and year\n",
    "    for data2 in dataset:\n",
    "        author2 = data2['author']\n",
    "        venue_dates2 = data2['venue_dates']\n",
    "\n",
    "        # Check if they published at the same venue in the same year\n",
    "        for venue, year in venue_dates.items():\n",
    "            if venue in venue_dates2 and venue_dates2[venue] == year:\n",
    "                if author != author2 and not G.has_edge(author, author2):\n",
    "                    # Calculate the weight of the edge\n",
    "                    common_publications = sum(1 for data in dataset if data['author'] in {author, author2} and venue in data['venues'] and data['venue_dates'][venue] == year)\n",
    "                    weight = round(common_publications / (num_papers[author] + num_papers[author2]), 4) * 100\n",
    "                    G.add_edge(author, author2, weight=weight)\n",
    "\n",
    "# Remove self-loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to the nodes to put them in teams.\n",
    "\n",
    "# Convert the database to a dictionary for faster lookup\n",
    "database_dict = {item['author']: item for item in dataset}\n",
    "\n",
    "for node in G.nodes():\n",
    "    # Get the corresponding item from the database\n",
    "    item = database_dict.get(node)\n",
    "    if item is not None:\n",
    "        # Assign the label to the node\n",
    "        G.nodes[node]['label'] = ps.assign_labels(item)\n",
    "\n",
    "# Remove nodes without a label attribute from the Graph\n",
    "nodes_without_label = [node for node in G.nodes if 'label' not in G.nodes[node]]\n",
    "G.remove_nodes_from(nodes_without_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the largest connected components of in each group to create the network.\n",
    "\n",
    "# Get the largest connected component for each label\n",
    "largest_components = {}\n",
    "for label in set(nx.get_node_attributes(G, 'label').values()):\n",
    "    label_nodes = [node for node, attr in G.nodes(data=True) if attr['label'] == label]\n",
    "    subgraph = G.subgraph(label_nodes)\n",
    "    largest_component = max(nx.connected_components(subgraph), key=len)\n",
    "    largest_components[label] = largest_component\n",
    "\n",
    "lcc = []\n",
    "# Print the largest connected component for each label\n",
    "for label, component in largest_components.items():\n",
    "    lcc.append({label: component})\n",
    "\n",
    "# Create a list of all the sets in the dictionaries\n",
    "result = [list(item.values())[0] for item in lcc]\n",
    "\n",
    "# Flatten the list of sets into a list of strings\n",
    "result = [item for sublist in result for item in sublist]\n",
    "\n",
    "G = G.subgraph(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save network G in a pickle file\n",
    "with open('./networks/network.pkl', 'wb') as file:\n",
    "    pickle.dump(G, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1503252"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10108"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
