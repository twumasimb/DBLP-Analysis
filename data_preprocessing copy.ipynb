{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import preprocessing as ps\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"./data/dblp_data.txt\"\n",
    "# # Process and save the data as a pickle file\n",
    "# ps.parse_and_save_paper_data(path, \"./data/dblp_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset = pickle.load(open(\"./data/dblp_data.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'OQL[C++]: Extending C++ with an Object Query Capability.',\n",
       " 'authors': ['José A. Blakeley'],\n",
       " 'year': 1995,\n",
       " 'venue': 'Modern Database Systems'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the data with all the values\n",
    "dataset = [data for data in dataset if all(value != '' for value in data.values())]\n",
    "dataset = [data for data in dataset if all(attr in data for attr in ['title', 'authors', 'year', 'venue'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29051"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample by venue - Only keep papers published in one of the following conference\n",
    "venues = ['VLDB', 'ICDE', 'ICDT', 'EDBT', 'PODS', 'SIGMOD Conference', 'ICML', 'ECML', 'COLT', 'UAI', 'SODA', 'STOC', 'FOCS', 'STACS', 'KDD', 'ICDM', 'PKDD', 'WWW', 'SDM']\n",
    "dataset = ps.sample_by_venue(dataset, venues)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10113"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data for each author\n",
    "authors = set()\n",
    "for data in dataset:\n",
    "    if 'authors' in data:\n",
    "        authors.update(data['authors'][0].split(','))\n",
    "\n",
    "unique_authors = list(authors)\n",
    "\n",
    "# Extract the author data\n",
    "dataset = ps.get_author_data(unique_authors, dataset)\n",
    "# clean empty values from the dataset\n",
    "dataset = [data for data in dataset if all(value != '' or None for value in data.values())]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 100,\n",
       " 'author': 'David A. McAllester',\n",
       " 'coauthors': ['John Langford',\n",
       "  'Yishay Mansour',\n",
       "  'Yishay Mansour',\n",
       "  'Robert E. Schapire',\n",
       "  'Vasant Shanbhogue',\n",
       "  'Prakash Panangaden',\n",
       "  'János A. Csirik',\n",
       "  'Robert E. Schapire',\n",
       "  'Michael L. Littman',\n",
       "  'Peter Stone',\n",
       "  'Henry A. Kautz',\n",
       "  'William W. Cohen',\n",
       "  'Satinder P. Singh',\n",
       "  'Fernando Pereira',\n",
       "  'Michael Collins',\n",
       "  'Deva Ramanan',\n",
       "  'Pedro F. Felzenszwalb',\n",
       "  'Ross B. Girshick',\n",
       "  'Petri Myllymäki'],\n",
       " 'venues': ['UAI', 'ICML', 'COLT', 'FOCS', 'KDD'],\n",
       " 'papers': {'Approximate Planning for Factored POMDPs using Belief State Simplification.',\n",
       "  'Boosting Using Branching Programs.',\n",
       "  'Case-Factor Diagrams for Structured Probabilistic Modeling.',\n",
       "  'Computable Shell Decomposition Bounds.',\n",
       "  'Discriminative Latent Variable Models for Object Detection.',\n",
       "  'Generalization Bounds for Decision Trees.',\n",
       "  'Hardening soft information sources.',\n",
       "  'Modeling Auction Price Uncertainty Using Boosting-based Conditional Density Estimation.',\n",
       "  'Nonexpressibility of Fairness and Signaling',\n",
       "  'On the Convergence Rate of Good-Turing Estimators.',\n",
       "  'PAC-Bayesian Model Averaging.',\n",
       "  'Simplified PAC-Bayesian Margin Bounds.',\n",
       "  'Some PAC-Bayesian Theorems.',\n",
       "  'UAI 2008, Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence, July 9-12, 2008, Helsinki, Finland'},\n",
       " 'num_of_papers': 14,\n",
       " 'venue_papers': {'COLT': 7, 'FOCS': 1, 'ICML': 2, 'KDD': 1, 'UAI': 3},\n",
       " 'venue_dates': {'COLT': 2000,\n",
       "  'FOCS': 1988,\n",
       "  'ICML': 2010,\n",
       "  'KDD': 2000,\n",
       "  'UAI': 2008}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a network of authors, add a weighted edge if they have authored the same paper or have published at the same conference. \n",
    "# Remove all self loops from the network. \n",
    "\n",
    "# Initialize an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Precompute the number of papers for each author\n",
    "num_papers = {author: ps.get_num_papers(dataset, author) for author in unique_authors}\n",
    "\n",
    "# Iterate over the dataset\n",
    "for data in dataset:\n",
    "    author = data['author']\n",
    "    coauthors = data['coauthors']\n",
    "    num_of_papers = data['num_of_papers']\n",
    "    venue_dates = data['venue_dates']\n",
    "\n",
    "    # Count the occurrences of each coauthor\n",
    "    coauthor_counts = Counter(coauthors)\n",
    "\n",
    "    # Add edges between the author and coauthors\n",
    "    for coauthor, count in coauthor_counts.items():\n",
    "        # Only put an edge between two nodes if they have two or more papers together\n",
    "        if count > 1:\n",
    "            # Calculate the weight of the edge\n",
    "            weight = round(count / (num_of_papers + num_papers[coauthor]), 4) * 100\n",
    "            if not G.has_edge(author, coauthor):\n",
    "                G.add_edge(author, coauthor, weight=weight)\n",
    "\n",
    "    # Add edges based on venue and year\n",
    "    for data2 in dataset:\n",
    "        author2 = data2['author']\n",
    "        venue_dates2 = data2['venue_dates']\n",
    "\n",
    "        # Check if they published at the same venue in the same year\n",
    "        for venue, year in venue_dates.items():\n",
    "            if venue in venue_dates2 and venue_dates2[venue] == year:\n",
    "                if author != author2:\n",
    "                    # Calculate the weight of the edge\n",
    "                    common_publications = sum(1 for data in dataset if data['author'] in {author, author2} and venue in data['venues'] and data['venue_dates'][venue] == year)\n",
    "                    weight = round(common_publications / (num_papers[author] + num_papers[author2]), 4) * 50\n",
    "                    \n",
    "                    if G.has_edge(author, author2):\n",
    "                        # If the edge exists, add the weight to the existing weight\n",
    "                        G[author][author2]['weight'] += weight\n",
    "                    else:\n",
    "                        # If the edge doesn't exist, create a new edge with the weight\n",
    "                        G.add_edge(author, author2, weight=weight)\n",
    "    \n",
    "\n",
    "    # Extract the weights from the graph\n",
    "    weights = [data['weight'] for _, _, data in G.edges(data=True)]\n",
    "\n",
    "    # Scale the weights between 0 and 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_weights = scaler.fit_transform([[weight] for weight in weights])\n",
    "\n",
    "    # Update the weights in the graph\n",
    "    for (u, v, data), scaled_weight in zip(G.edges(data=True), scaled_weights):\n",
    "        data['weight'] = scaled_weight[0]\n",
    "# Remove self-loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to the nodes to put them in teams.\n",
    "G = G.copy()\n",
    "# Convert the database to a dictionary for faster lookup\n",
    "database_dict = {item['author']: item for item in dataset}\n",
    "\n",
    "for node in G.nodes():\n",
    "    # Get the corresponding item from the database\n",
    "    item = database_dict.get(node)\n",
    "    if item is not None:\n",
    "        # Assign the label to the node\n",
    "        G.nodes[node]['label'] = ps.assign_labels(item)\n",
    "\n",
    "# Remove nodes without a label attribute from the Graph\n",
    "nodes_without_label = [node for node in G.nodes if 'label' not in G.nodes[node]]\n",
    "G.remove_nodes_from(nodes_without_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the largest connected components of in each group to create the network.\n",
    "\n",
    "# Get the largest connected component for each label\n",
    "largest_components = {}\n",
    "for label in set(nx.get_node_attributes(G, 'label').values()):\n",
    "    label_nodes = [node for node, attr in G.nodes(data=True) if attr['label'] == label]\n",
    "    subgraph = G.subgraph(label_nodes)\n",
    "    largest_component = max(nx.connected_components(subgraph), key=len)\n",
    "    largest_components[label] = largest_component\n",
    "\n",
    "lcc = []\n",
    "# Print the largest connected component for each label\n",
    "for label, component in largest_components.items():\n",
    "    lcc.append({label: component})\n",
    "\n",
    "# Create a list of all the sets in the dictionaries\n",
    "result = [list(item.values())[0] for item in lcc]\n",
    "\n",
    "# Flatten the list of sets into a list of strings\n",
    "result = [item for sublist in result for item in sublist]\n",
    "\n",
    "G = G.subgraph(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save network G in a pickle file\n",
    "with open('./networks/network_2.pkl', 'wb') as file:\n",
    "    pickle.dump(G, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1503252"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of edges compared to fully connected network: 0.0147\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ratio of edges compared to fully connected network: {round(G.number_of_edges() /(G.number_of_nodes()**2), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102171664"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10108 * 10108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1503252/102171664, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subgraph nodes per team of 10, 20, 30, 40, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# dataset = pickle.load(open('./networks/network.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10, 51, 10):\n",
    "#     network = ps.create_subnet(dataset.copy(), i).copy()\n",
    "\n",
    "#     # Save as pickle\n",
    "#     output_file = f\"./networks/subnets/{i}_nodes.pkl\"\n",
    "#     with open(output_file, 'wb') as pickle_file:\n",
    "#         pickle.dump(network, pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
